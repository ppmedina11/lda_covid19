{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18959e1f-0bff-4409-be6d-096bbcff4b6e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div id=\"data-sources-description\" style=\"background-color:#80CDC6 ; padding: 10px 0;\">\n",
    "    <center><h1 style=\"color:#2F3254; font-weight:bold\">PRELIMINARIES</h1></center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e198aaa-ac9c-4d29-82b5-63055b659792",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T08:00:56.894043Z",
     "iopub.status.busy": "2024-06-12T08:00:56.893793Z",
     "iopub.status.idle": "2024-06-12T08:01:20.484301Z",
     "shell.execute_reply": "2024-06-12T08:01:20.483742Z",
     "shell.execute_reply.started": "2024-06-12T08:00:56.894020Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29da402741a74d5296de2ca731723a6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody><tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>2</td><td>application_1718175985716_0003</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-86-6.ec2.internal:20888/proxy/application_1718175985716_0003/\" class=\"emr-proxy-link j-FU9KO7CVBUSR application_1718175985716_0003\" emr-resource=\"j-FU9KO7CVBUSR\n",
       "\" application-id=\"application_1718175985716_0003\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-89-232.ec2.internal:8042/node/containerlogs/container_1718175985716_0003_01_000002/livy\">Link</a></td><td>None</td><td>✔</td></tr></tbody></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=yarn appName=livy-session-2>"
     ]
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "225d04c7-a063-43f6-b672-264240584f18",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-06-12T08:01:21.861694Z",
     "iopub.status.busy": "2024-06-12T08:01:21.861523Z",
     "iopub.status.idle": "2024-06-12T08:01:43.190207Z",
     "shell.execute_reply": "2024-06-12T08:01:43.189618Z",
     "shell.execute_reply.started": "2024-06-12T08:01:21.861674Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bb7c7cb508d4635b67420bc50486248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Downloading pyarrow-16.1.0-cp39-cp39-manylinux_2_28_x86_64.whl (40.8 MB)\n",
      "Collecting numpy>=1.16.6\n",
      "  Downloading numpy-1.26.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Installing collected packages: numpy, pyarrow\n",
      "Successfully installed numpy-1.26.4 pyarrow-16.1.0\n",
      "\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.9.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.53.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "Collecting pillow>=8\n",
      "  Downloading pillow-10.3.0-cp39-cp39-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "Collecting kiwisolver>=1.3.1\n",
      "  Downloading kiwisolver-1.4.5-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting packaging>=20.0\n",
      "  Downloading packaging-24.1-py3-none-any.whl (53 kB)\n",
      "Collecting pyparsing>=2.3.1\n",
      "  Downloading pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "Collecting importlib-resources>=3.2.0\n",
      "  Downloading importlib_resources-6.4.0-py3-none-any.whl (38 kB)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.2.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (304 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in ./tmp/spark-f8b16435-9b24-4ceb-854a-99e7a5523328/lib64/python3.9/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/lib/python3.9/site-packages (from matplotlib) (2.8.1)\n",
      "Collecting zipp>=3.1.0\n",
      "  Downloading zipp-3.19.2-py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.13.0)\n",
      "Installing collected packages: zipp, pyparsing, pillow, packaging, kiwisolver, importlib-resources, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.2.1 cycler-0.12.1 fonttools-4.53.0 importlib-resources-6.4.0 kiwisolver-1.4.5 matplotlib-3.9.0 packaging-24.1 pillow-10.3.0 pyparsing-3.1.2 zipp-3.19.2\n",
      "\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "Collecting tzdata>=2022.7\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Collecting python-dateutil>=2.8.2\n",
      "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: numpy>=1.22.4 in ./tmp/spark-f8b16435-9b24-4ceb-854a-99e7a5523328/lib64/python3.9/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.13.0)\n",
      "Installing collected packages: tzdata, python-dateutil, pandas\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.8.1\n",
      "    Not uninstalling python-dateutil at /usr/lib/python3.9/site-packages, outside environment /mnt1/yarn/usercache/livy/appcache/application_1718175985716_0003/container_1718175985716_0003_01_000002/tmp/spark-f8b16435-9b24-4ceb-854a-99e7a5523328\n",
      "    Can't uninstall 'python-dateutil'. No files were found to uninstall.\n",
      "Successfully installed pandas-2.2.2 python-dateutil-2.9.0.post0 tzdata-2024.1\n",
      "\n",
      "Requirement already satisfied: numpy in ./tmp/spark-f8b16435-9b24-4ceb-854a-99e7a5523328/lib64/python3.9/site-packages (1.26.4)\n",
      "\n",
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.9/site-packages (from langdetect) (1.13.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=b873feb4ee626eb8c6a72af614165484f0f1f625aa03fae4c0f3e17eaf12b762\n",
      "  Stored in directory: /mnt/tmp/pip-ephem-wheel-cache-muocw3hy/wheels/d1/c1/d9/7e068de779d863bc8f8fc9467d85e25cfe47fa5051fff1a1bb\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 2.15.30 requires python-dateutil<=2.8.2,>=2.1, but you have python-dateutil 2.9.0.post0 which is incompatible.\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag."
     ]
    }
   ],
   "source": [
    "sc.install_pypi_package('pyarrow')\n",
    "sc.install_pypi_package('matplotlib')\n",
    "sc.install_pypi_package('pandas')\n",
    "sc.install_pypi_package('numpy')\n",
    "sc.install_pypi_package('langdetect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73af2d5b-f385-4a39-860e-d834d4a4d589",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T08:02:01.497048Z",
     "iopub.status.busy": "2024-06-12T08:02:01.496875Z",
     "iopub.status.idle": "2024-06-12T08:02:01.534860Z",
     "shell.execute_reply": "2024-06-12T08:02:01.534218Z",
     "shell.execute_reply.started": "2024-06-12T08:02:01.497028Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cba636aed744bdebf958542fa1d1c48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.master(\"yarn\")\n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"org.apache.hadoop:hadoop-aws:3.3.4\", # should match hadoop version\n",
    "    )  \n",
    "    .config(\n",
    "        \"spark.executor.extraJavaOptions\",\n",
    "        \"-Dcom.amazonaws.services.s3.enableV4=true\",\n",
    "    )\n",
    "    .config(\n",
    "        \"spark.driver.extraJavaOptions\",\n",
    "        \"-Dcom.amazonaws.services.s3.enableV4=true\",\n",
    "    )\n",
    "    .config(\n",
    "        \"spark.hadoop.fs.s3a.aws.credentials.provider\",\n",
    "        'com.amazonaws.auth.profile.ProfileCredentialsProvider,'\n",
    "        \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\",\n",
    "    )\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)  # Enables better visualization of DataFrames in notebooks\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "971bd0bd-0dfe-405a-b9ba-4b1c10cf16f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T08:02:02.342087Z",
     "iopub.status.busy": "2024-06-12T08:02:02.341910Z",
     "iopub.status.idle": "2024-06-12T08:02:02.381373Z",
     "shell.execute_reply": "2024-06-12T08:02:02.380872Z",
     "shell.execute_reply.started": "2024-06-12T08:02:02.342067Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9c4c0e3b4504801b5feeb646fc9764b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba5c4196-1abf-4144-a7c8-581c32609359",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T08:02:03.181567Z",
     "iopub.status.busy": "2024-06-12T08:02:03.181400Z",
     "iopub.status.idle": "2024-06-12T08:02:04.435212Z",
     "shell.execute_reply": "2024-06-12T08:02:04.434449Z",
     "shell.execute_reply.started": "2024-06-12T08:02:03.181548Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af496ca4d93740899478e4b16031ea79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pyspark.sql import functions as F\n",
    "from langdetect import detect\n",
    "from langdetect import DetectorFactory\n",
    "from langdetect import detect_langs\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ae3a7d-4ea1-42ec-a1d0-8c17680d003e",
   "metadata": {},
   "source": [
    "<div id=\"data-sources-description\" style=\"background-color:#80CDC6 ; padding: 10px 0;\">\n",
    "    <center><h1 style=\"color:#2F3254; font-weight:bold\">DATA CLEANING AND FILTERING</h1></center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922ea17d-ab34-4e6f-9ff9-0f30fc3b1973",
   "metadata": {},
   "source": [
    "For this study, only the metadata files were used to implement topic modelling. Each row of the metadata file corresponds to one paper object. For each metadata file, **only the abstracts were analyzed**. The dataset cleaning and filtering process for the abstracts is as follows:\n",
    "\n",
    "1. **Reading Data:**\n",
    "The dataset was read from an S3 bucket, specifically from the AI2 Semantic Scholar CORD-19 dataset, which is stored in CSV format. The schema of the dataset was inferred automatically, and headers were used for column names.\n",
    "\n",
    "2. **Extracting Date Components:**\n",
    "Two new columns, `year` and `month`, were created by extracting the year and month from the 'publish_time' column. This involved converting the `publish_time` to a date format initially. These information were later used for writing parquet files to improve the execution time of downstream implementations.\n",
    "\n",
    "3. **Filtering by Year:**\n",
    "The dataset was filtered to only include documents published between 2018 and 2024, with the intent of partitioning the research narrative into three phases: pre-COVID publications before the pandemic emerged, early COVID publications from the initial outbreak period, and late COVID publications covering research conducted after the pandemic was underway. Dividing the literature this way allows for conducting analysis across the evolving timeline of the COVID-19 crisis. This facilitates studying how scientific understanding progressed from baseline coronavirus knowledge to the first implications of the outbreak to later pandemic research.\n",
    "\n",
    "4. **Lowercasing and Aliasing:**\n",
    "The `year`, `month`, `title`, `abstract`, and `journal` columns were converted to lowercase to maintain consistency and avoid duplicates arising from case sensitivity. Each of these fields was then aliased to preserve their original names but in lowercase, enhancing the uniformity of the data.\n",
    "\n",
    "5. **Null and Quality Checks on Abstracts:**\n",
    "Entries where the `abstract` was null, less than 100 characters, or contained placeholder texts like 'null' or 'unknown' were excluded. This step ensures that only meaningful and substantial abstracts are retained for analysis.\n",
    "\n",
    "6. **Title and Date Validity Checks:**\n",
    "The dataset was further refined by removing any records missing titles, or where `year` or `month` data were missing. This guarantees that the remaining records have complete date and title information.\n",
    "\n",
    "7. **Removing Duplicates:**\n",
    "Duplicate entries based on `title` and `abstract` were removed to prevent redundancy in the dataset. This step is crucial for maintaining the integrity of any subsequent analyses.\n",
    "\n",
    "8. **Caching:**\n",
    "To optimize performance for downstream operations, the resulting DataFrame was cached. This avoids re-computation of the DataFrame in subsequent actions and speeds up the data processing workflow.\n",
    "\n",
    "9. **Language Detection and Filtering:**\n",
    "A custom function `get_most_probable_language` was used to detect the most probable language for each abstract, retaining only those written in English.\n",
    "\n",
    "10. **Saving as Parquet Files:**\n",
    "After filtering and cleaning, the dataset was saved as Parquet files, partitioned by year and month. This storage format reduces the file size but also optimizes read and write efficiency when handling large datasets. Note that the partitioning by month was due to the initial intention of implementing topic modelling bi-annually. In the end, the researchers decided to do an annual analysis to capture a broader set of themes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a535d4d-772c-4bfc-b5ce-4344905a1f13",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-06-12T08:02:27.527038Z",
     "iopub.status.busy": "2024-06-12T08:02:27.526866Z",
     "iopub.status.idle": "2024-06-12T08:05:51.373538Z",
     "shell.execute_reply": "2024-06-12T08:05:51.372995Z",
     "shell.execute_reply.started": "2024-06-12T08:02:27.527019Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8da74796b2a943e2bb510e49eb3c1c8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_meta = (spark\n",
    "           .read\n",
    "           .csv('s3://ai2-semanticscholar-cord-19/????-??-??/metadata.csv',\n",
    "                header=True, inferSchema=True)\n",
    "           .withColumn('year', F.year(F.to_date( F.col('publish_time'))))\n",
    "           .withColumn('month', F.month(F.to_date(F.col('publish_time'))))\n",
    "           .filter(F.col('year').isin(2018, 2019, 2020, 2021, 2022, 2023,\n",
    "                                      2024))\n",
    "           .select(F.lower(F.col('year')).alias('year'),\n",
    "                   F.lower(F.col('month')).alias('month'),\n",
    "                   F.lower(F.col('title')).alias('title'),\n",
    "                   F.lower(F.col('abstract')).alias('abstract'),\n",
    "                   F.lower(F.col('journal')).alias('journal')\n",
    "                  )\n",
    "           .filter((F.col('abstract').isNotNull()) &\n",
    "                   (F.length(F.col('abstract')) > 100) &\n",
    "                   (F.col('abstract') != 'null') &\n",
    "                   (F.col('abstract') != 'unknown') &\n",
    "                   (F.col('title').isNotNull()) &\n",
    "                   (F.col('year').isNotNull()) &\n",
    "                   (F.col('month').isNotNull())\n",
    "                  )\n",
    "           .dropDuplicates(['title', 'abstract'])\n",
    "           .cache()\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4e3685-1ad0-4c5e-901e-43833a6c2abd",
   "metadata": {},
   "source": [
    "<div id=\"data-sources-description\" style=\"background-color:#2F3254 ; padding: 10px 0;\">\n",
    "    <center><h3 style=\"color:white; font-weight:bold\">Select English Abstracts</h3></center>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77e66d19-25f8-45aa-8010-f018491072f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T08:05:55.432514Z",
     "iopub.status.busy": "2024-06-12T08:05:55.432343Z",
     "iopub.status.idle": "2024-06-12T08:05:55.673851Z",
     "shell.execute_reply": "2024-06-12T08:05:55.673314Z",
     "shell.execute_reply.started": "2024-06-12T08:05:55.432494Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "284b634be88043fb8c2baec9748ad708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reproducibility\n",
    "DetectorFactory.seed = 42\n",
    "\n",
    "def get_most_probable_language(abstract):\n",
    "    \"\"\"\n",
    "    Detect and return the most probable language of the abstract.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    abstract : str\n",
    "        The abstract for which to detect the language.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The most probable language represented by its ISO code.\n",
    "        Returns 'unknown' if language detection fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get a list of detected languages with their probabilities\n",
    "        languages = detect_langs(abstract)\n",
    "        # Sort the list based on probability\n",
    "        # Then select the most probable language\n",
    "        if languages:\n",
    "            most_probable_language = max(languages,\n",
    "                                         key=lambda lang: lang.prob)\n",
    "            return most_probable_language.lang\n",
    "    except:\n",
    "        return \"unknown\"\n",
    "\n",
    "# Register the UDF with a return type of StringType\n",
    "get_language_udf = F.udf(get_most_probable_language, StringType())\n",
    "\n",
    "# Apply the UDF to the 'abstract' column to detect languages\n",
    "df_meta = df_meta.withColumn(\"language\", get_language_udf(F.col(\"abstract\")))\n",
    "\n",
    "# Only get the abstracts that are in English\n",
    "df_meta = df_meta.filter(F.col('language') == 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3efe1753-a4d5-4e06-8388-e4764a01e101",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T08:05:56.760376Z",
     "iopub.status.busy": "2024-06-12T08:05:56.760210Z",
     "iopub.status.idle": "2024-06-12T08:05:56.797127Z",
     "shell.execute_reply": "2024-06-12T08:05:56.796646Z",
     "shell.execute_reply.started": "2024-06-12T08:05:56.760357Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f990186e73e245669dc8df970eee1bb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- abstract: string (nullable = true)\n",
      " |-- journal: string (nullable = true)\n",
      " |-- language: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "df_abstract.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2606e18-69cc-4e6f-bf89-6417fba9b873",
   "metadata": {},
   "source": [
    "<div id=\"data-sources-description\" style=\"background-color:#2F3254 ; padding: 10px 0;\">\n",
    "    <center><h3 style=\"color:white; font-weight:bold\">Save Filtered Dataset as Parquet Files</h3></center>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62fdafa-99bf-4c83-8108-f9c6d1e97bff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write DataFrame to S3, partitioned by year and month\n",
    "(df_meta\n",
    " .write\n",
    " .partitionBy(\"year\", \"month\")\n",
    " .parquet(\"s3://bdcc-project/parquet/\", mode='overwrite'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25568da-d0c9-4e3b-a7a5-f8022e1faf2b",
   "metadata": {},
   "source": [
    "<div id=\"data-sources-description\" style=\"background-color:#2F3254 ; padding: 10px 0;\">\n",
    "    <center><h3 style=\"color:white; font-weight:bold\">Load Final Dataset</h3></center>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a9f1015-2a64-458b-8656-71ca741ddd51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T08:07:44.256364Z",
     "iopub.status.busy": "2024-06-12T08:07:44.256173Z",
     "iopub.status.idle": "2024-06-12T08:07:53.530532Z",
     "shell.execute_reply": "2024-06-12T08:07:53.530022Z",
     "shell.execute_reply.started": "2024-06-12T08:07:44.256344Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa9b6e19e83344bd8643bc5234da1f8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_abstract = (spark\n",
    "               .read\n",
    "               .parquet('s3://bdcc-project/parquet/',\n",
    "                        inferSchema=True)\n",
    "               .cache()\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc703c7b-44d2-4124-b9b3-d3fbb02a14d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T08:09:19.463285Z",
     "iopub.status.busy": "2024-06-12T08:09:19.463112Z",
     "iopub.status.idle": "2024-06-12T08:09:20.712423Z",
     "shell.execute_reply": "2024-06-12T08:09:20.711746Z",
     "shell.execute_reply.started": "2024-06-12T08:09:19.463266Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c37c1e6c3077425fbe9f46b476553016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of documents per year:\n",
      "+----+------+\n",
      "|year| count|\n",
      "+----+------+\n",
      "|2018|  8214|\n",
      "|2019|  9386|\n",
      "|2020|289964|\n",
      "|2021|382923|\n",
      "|2022|115118|\n",
      "|2023|    17|\n",
      "+----+------+"
     ]
    }
   ],
   "source": [
    "print('Count of documents per year:')\n",
    "(df_abstract\n",
    " .filter(F.col('year').isNotNull())\n",
    " .groupBy('year')\n",
    " .count()\n",
    " .orderBy('year', ascending=True)\n",
    " .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609432dc-4a18-41cd-9c3c-379828416d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
